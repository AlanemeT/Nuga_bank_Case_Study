{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bde5e8b",
   "metadata": {},
   "source": [
    "### Import Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecb2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6d76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our spark session\n",
    "spark = SparkSession.builder.appName('NugaBankETL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607434a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-IVUNJ9J:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NugaBankETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14ba075b320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc25ce",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcab6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuga_bank_df = spark.read.csv(r'dataset\\nuga_bank_transactions.csv', header=True, inferSchema=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b24ef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|    Transaction_Date|Amount|Transaction_Type| Customer_Name|    Customer_Address|     Customer_City|Customer_State|    Customer_Country|             Company|           Job_Title|               Email|       Phone_Number|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|Gender|Marital_Status|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|2024-03-23 15:38:...| 34.76|      Withdrawal|    James Neal|54912 Holmes Lodg...| West Keithborough|       Florida|                Togo|Benson, Johnson a...|                NULL|                NULL|  493.720.6609x7545|  3592901394693441|GB98RBPP090285271...|          MAD|       3167.0|       C|    Z|       No|2020-06-20 03:04:...|Yeah food anythin...| Other|      Divorced|\n",
      "|2024-04-22 19:15:...|163.92|      Withdrawal|   Thomas Long| 1133 Collin Passage|        Joshuabury|   Connecticut|Lao People's Demo...|                NULL|   Food technologist|michellelynch@exa...|      (497)554-3317|              NULL|GB03KFZR339662263...|          VEF|       2122.0|       B|    Z|     NULL|2020-12-27 13:23:...|Teach edge make n...|Female|       Married|\n",
      "|2024-04-12 19:46:...|386.32|      Withdrawal|Ashley Shelton|5297 Johnson Port...|       North Maria|    New Jersey|              Bhutan|       Jones-Mueller|Database administ...| ljordan@example.org|      (534)769-3072|      675983949974|GB59QYRN446730519...|          COP|       7796.0|       C|    Z|       No|2020-01-24 01:23:...|Again line face c...| Other|          NULL|\n",
      "|2024-04-17 15:29:...|407.15|         Deposit| James Rosario|56955 Moore Glens...|North Michellefurt|    New Mexico|             Iceland|       Vargas-Harris|Horticultural the...|parkerjames@examp...|+1-447-900-1320x257|     4761202519057|GB74FTDO268299438...|          BWP|       6284.0|       C|    Z|      Yes|2023-09-27 03:01:...|     Bag my a drive.|  NULL|          NULL|\n",
      "|2024-02-10 01:51:...|161.31|         Deposit|Miguel Leonard|262 Beck Expressw...|              NULL| West Virginia|             Eritrea|Richardson, Gonza...|   Minerals surveyor| zweaver@example.net|               NULL|   213156729655186|GB94EWRN587847592...|          SOS|       9179.0|       C|    Y|       No|2022-01-22 19:08:...|Husband find ok w...|Female|       Married|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "nuga_bank_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fd153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Transaction_Type: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Customer_Address: string (nullable = true)\n",
      " |-- Customer_City: string (nullable = true)\n",
      " |-- Customer_State: string (nullable = true)\n",
      " |-- Customer_Country: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Job_Title: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone_Number: string (nullable = true)\n",
      " |-- Credit_Card_Number: long (nullable = true)\n",
      " |-- IBAN: string (nullable = true)\n",
      " |-- Currency_Code: string (nullable = true)\n",
      " |-- Random_Number: double (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- Is_Active: string (nullable = true)\n",
      " |-- Last_Updated: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nuga_bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35532ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Date',\n",
       " 'Amount',\n",
       " 'Transaction_Type',\n",
       " 'Customer_Name',\n",
       " 'Customer_Address',\n",
       " 'Customer_City',\n",
       " 'Customer_State',\n",
       " 'Customer_Country',\n",
       " 'Company',\n",
       " 'Job_Title',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Credit_Card_Number',\n",
       " 'IBAN',\n",
       " 'Currency_Code',\n",
       " 'Random_Number',\n",
       " 'Category',\n",
       " 'Group',\n",
       " 'Is_Active',\n",
       " 'Last_Updated',\n",
       " 'Description',\n",
       " 'Gender',\n",
       " 'Marital_Status']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuga_bank_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0028f72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of rows\n",
    "num_rows = nuga_bank_df.count()\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28363a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of columns\n",
    "num_columns = len(nuga_bank_df.columns)\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17929c16",
   "metadata": {},
   "source": [
    "### Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d1da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Nulls 0\n",
      "Amount Nulls 0\n",
      "Transaction_Type Nulls 0\n",
      "Customer_Name Nulls 100425\n",
      "Customer_Address Nulls 100087\n",
      "Customer_City Nulls 100034\n",
      "Customer_State Nulls 100009\n",
      "Customer_Country Nulls 100672\n",
      "Company Nulls 100295\n",
      "Job_Title Nulls 99924\n",
      "Email Nulls 100043\n",
      "Phone_Number Nulls 100524\n",
      "Credit_Card_Number Nulls 100085\n",
      "IBAN Nulls 100300\n",
      "Currency_Code Nulls 99342\n",
      "Random_Number Nulls 99913\n",
      "Category Nulls 100332\n",
      "Group Nulls 100209\n",
      "Is_Active Nulls 100259\n",
      "Last_Updated Nulls 100321\n",
      "Description Nulls 100403\n",
      "Gender Nulls 99767\n",
      "Marital_Status Nulls 99904\n"
     ]
    }
   ],
   "source": [
    "# Checking for nul values\n",
    "for column in nuga_bank_df.columns:\n",
    "    print(column, 'Nulls', nuga_bank_df.filter(nuga_bank_df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33f65a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to fill up missing values\n",
    "nuga_bank_df_clean = nuga_bank_df.fillna({\n",
    "    'Customer_Name': 'Unknown',\n",
    "    'Customer_Address': 'Unknown',\n",
    "    'Customer_City': 'Unknown',\n",
    "    'Customer_State': 'Unknown',\n",
    "    'Customer_Country': 'Unknown',\n",
    "    'Company': 'Unknown',\n",
    "    'Job_Title': 'Unknown',\n",
    "    'Email': 'Unknown',\n",
    "    'Phone_Number': 'Unknown',\n",
    "    'Credit_Card_Number': 0,\n",
    "    'IBAN': 'Unknown',\n",
    "    'Currency_Code': 'Unknown',\n",
    "    'Random_Number': 0.0,\n",
    "    'Category': 'Unknown',\n",
    "    'Group': 'Unknown',\n",
    "    'Is_Active': 'Unknown',\n",
    "    'Description': 'Unknown',\n",
    "    'Gender': 'Unknown',\n",
    "    'Marital_Status': 'Unknown'\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61f87e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Nulls 0\n",
      "Amount Nulls 0\n",
      "Transaction_Type Nulls 0\n",
      "Customer_Name Nulls 0\n",
      "Customer_Address Nulls 0\n",
      "Customer_City Nulls 0\n",
      "Customer_State Nulls 0\n",
      "Customer_Country Nulls 0\n",
      "Company Nulls 0\n",
      "Job_Title Nulls 0\n",
      "Email Nulls 0\n",
      "Phone_Number Nulls 0\n",
      "Credit_Card_Number Nulls 0\n",
      "IBAN Nulls 0\n",
      "Currency_Code Nulls 0\n",
      "Random_Number Nulls 0\n",
      "Category Nulls 0\n",
      "Group Nulls 0\n",
      "Is_Active Nulls 0\n",
      "Last_Updated Nulls 100321\n",
      "Description Nulls 0\n",
      "Gender Nulls 0\n",
      "Marital_Status Nulls 0\n"
     ]
    }
   ],
   "source": [
    "# Confirm if no null values\n",
    "for column in nuga_bank_df_clean.columns:\n",
    "    print(column, 'Nulls', nuga_bank_df_clean.filter(nuga_bank_df_clean[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f85b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where Last_Updated is Null\n",
    "nuga_bank_df_clean = nuga_bank_df_clean.na.drop(subset=['Last_Updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2bdb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Nulls 0\n",
      "Amount Nulls 0\n",
      "Transaction_Type Nulls 0\n",
      "Customer_Name Nulls 0\n",
      "Customer_Address Nulls 0\n",
      "Customer_City Nulls 0\n",
      "Customer_State Nulls 0\n",
      "Customer_Country Nulls 0\n",
      "Company Nulls 0\n",
      "Job_Title Nulls 0\n",
      "Email Nulls 0\n",
      "Phone_Number Nulls 0\n",
      "Credit_Card_Number Nulls 0\n",
      "IBAN Nulls 0\n",
      "Currency_Code Nulls 0\n",
      "Random_Number Nulls 0\n",
      "Category Nulls 0\n",
      "Group Nulls 0\n",
      "Is_Active Nulls 0\n",
      "Last_Updated Nulls 0\n",
      "Description Nulls 0\n",
      "Gender Nulls 0\n",
      "Marital_Status Nulls 0\n"
     ]
    }
   ],
   "source": [
    "# Confirm o null values for Last_Updated\n",
    "for column in nuga_bank_df_clean.columns:\n",
    "    print(column, 'Nulls', nuga_bank_df_clean.filter(nuga_bank_df_clean[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8e00cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899679"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm the no of rows again after row drop for Last_Updated\n",
    "num_rows = nuga_bank_df_clean.count()\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98816571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+------------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "|summary|            Amount|Transaction_Type|Customer_Name|    Customer_Address|Customer_City|Customer_State|Customer_Country|      Company|         Job_Title|              Email|        Phone_Number|  Credit_Card_Number|                IBAN|Currency_Code|     Random_Number|Category|  Group|Is_Active|         Description| Gender|Marital_Status|\n",
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+------------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "|  count|            899679|          899679|       899679|              899679|       899679|        899679|          899679|       899679|            899679|             899679|              899679|              899679|              899679|       899679|            899679|  899679| 899679|   899679|              899679| 899679|        899679|\n",
      "|   mean|505.10367708927134|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL| 6.002851385999521E9|3.409662226750519E17|                NULL|         NULL| 4952.920380491264|    NULL|   NULL|     NULL|                NULL|   NULL|          NULL|\n",
      "| stddev|  285.794534330047|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL|2.3068371038619423E9|1.189655005733002...|                NULL|         NULL|2966.5434011515995|    NULL|   NULL|     NULL|                NULL|   NULL|          NULL|\n",
      "|    min|              10.0|         Deposit| Aaron Abbott|000 Aaron Landing...|    Aaronberg|       Alabama|     Afghanistan| Abbott Group|Academic librarian|            Unknown|       (200)201-4254|                   0|GB02AAAU191993009...|          AED|               0.0|       A|Unknown|       No|A American and to...| Female|      Divorced|\n",
      "|    max|            1000.0|      Withdrawal|    Zoe Young|             Unknown|  Zunigaville|       Wyoming|        Zimbabwe|Zuniga-Wilson|      Youth worker|zzuniga@example.org|             Unknown| 4999984361512569455|             Unknown|          ZWD|            9999.0| Unknown|      Z|      Yes|Yourself young ev...|Unknown|       Unknown|\n",
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+------------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To view the summary statistics of the data\n",
    "nuga_bank_df_clean.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058ebae",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88fad1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Date',\n",
       " 'Amount',\n",
       " 'Transaction_Type',\n",
       " 'Customer_Name',\n",
       " 'Customer_Address',\n",
       " 'Customer_City',\n",
       " 'Customer_State',\n",
       " 'Customer_Country',\n",
       " 'Company',\n",
       " 'Job_Title',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Credit_Card_Number',\n",
       " 'IBAN',\n",
       " 'Currency_Code',\n",
       " 'Random_Number',\n",
       " 'Category',\n",
       " 'Group',\n",
       " 'Is_Active',\n",
       " 'Last_Updated',\n",
       " 'Description',\n",
       " 'Gender',\n",
       " 'Marital_Status']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuga_bank_df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7c65c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+\n",
      "|    Transaction_Date|Amount|Transaction_Type|\n",
      "+--------------------+------+----------------+\n",
      "|2024-01-10 22:13:...|169.64|         Deposit|\n",
      "|2024-01-06 12:05:...|444.53|         Deposit|\n",
      "|2024-01-09 02:32:...|976.36|        Transfer|\n",
      "|2024-02-18 21:04:...|521.62|         Deposit|\n",
      "|2024-04-11 13:35:...|416.11|         Deposit|\n",
      "|2024-03-20 11:34:...|438.03|         Deposit|\n",
      "|2024-04-29 10:42:...| 28.27|        Transfer|\n",
      "|2024-02-12 15:48:...|657.39|         Deposit|\n",
      "|2024-01-16 03:08:...|489.04|      Withdrawal|\n",
      "|2024-04-27 01:11:...| 32.36|      Withdrawal|\n",
      "|2024-04-13 04:39:...| 152.8|         Deposit|\n",
      "|2024-02-07 20:31:...|736.03|      Withdrawal|\n",
      "|2024-03-09 11:50:...|516.88|        Transfer|\n",
      "|2024-01-22 16:12:...|615.23|         Deposit|\n",
      "|2024-02-18 19:51:...|119.83|        Transfer|\n",
      "|2024-04-15 10:58:...|630.29|         Deposit|\n",
      "|2024-02-22 06:42:...|923.79|         Deposit|\n",
      "|2024-04-22 06:27:...|832.71|      Withdrawal|\n",
      "|2024-02-14 10:28:...| 73.65|        Transfer|\n",
      "|2024-01-20 09:52:...| 850.8|      Withdrawal|\n",
      "+--------------------+------+----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# transaction table\n",
    "transaction = nuga_bank_df_clean.select('Transaction_Date', 'Amount', 'Transaction_Type').distinct()\n",
    "transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2c8f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97de897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the transaction_id column\n",
    "transaction = transaction.withColumn('transaction_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "545e33e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+--------------+\n",
      "|    Transaction_Date|Amount|Transaction_Type|transaction_id|\n",
      "+--------------------+------+----------------+--------------+\n",
      "|2024-03-23 15:38:...| 34.76|      Withdrawal|             0|\n",
      "|2024-04-22 19:15:...|163.92|      Withdrawal|             1|\n",
      "|2024-04-12 19:46:...|386.32|      Withdrawal|             2|\n",
      "|2024-04-17 15:29:...|407.15|         Deposit|             3|\n",
      "|2024-02-10 01:51:...|161.31|         Deposit|             4|\n",
      "|2024-02-10 22:56:...|764.34|        Transfer|             5|\n",
      "|2024-04-07 00:07:...|734.59|         Deposit|             6|\n",
      "|2024-03-08 01:51:...|592.43|         Deposit|             7|\n",
      "|2024-02-01 12:34:...| 927.1|         Deposit|             8|\n",
      "|2024-03-22 16:46:...| 66.59|        Transfer|             9|\n",
      "|2024-04-23 13:30:...| 246.3|      Withdrawal|            10|\n",
      "|2024-01-13 01:22:...|782.32|      Withdrawal|            11|\n",
      "|2024-02-25 15:16:...|818.42|      Withdrawal|            12|\n",
      "|2024-01-01 20:55:...|352.23|      Withdrawal|            13|\n",
      "|2024-01-19 00:01:...|316.19|      Withdrawal|            14|\n",
      "|2024-04-09 14:40:...|662.26|      Withdrawal|            15|\n",
      "|2024-04-15 04:58:...|893.73|         Deposit|            16|\n",
      "|2024-04-12 14:32:...|746.22|      Withdrawal|            17|\n",
      "|2024-02-26 14:43:...|214.77|      Withdrawal|            18|\n",
      "|2024-01-17 14:50:...|726.94|      Withdrawal|            19|\n",
      "+--------------------+------+----------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3da96d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+----------------+\n",
      "|transaction_id|    Transaction_Date|Amount|Transaction_Type|\n",
      "+--------------+--------------------+------+----------------+\n",
      "|             0|2024-01-10 22:13:...|169.64|         Deposit|\n",
      "|             1|2024-01-06 12:05:...|444.53|         Deposit|\n",
      "|             2|2024-01-09 02:32:...|976.36|        Transfer|\n",
      "|             3|2024-02-18 21:04:...|521.62|         Deposit|\n",
      "|             4|2024-04-11 13:35:...|416.11|         Deposit|\n",
      "|             5|2024-03-20 11:34:...|438.03|         Deposit|\n",
      "|             6|2024-04-29 10:42:...| 28.27|        Transfer|\n",
      "|             7|2024-02-12 15:48:...|657.39|         Deposit|\n",
      "|             8|2024-01-16 03:08:...|489.04|      Withdrawal|\n",
      "|             9|2024-04-27 01:11:...| 32.36|      Withdrawal|\n",
      "|            10|2024-04-13 04:39:...| 152.8|         Deposit|\n",
      "|            11|2024-02-07 20:31:...|736.03|      Withdrawal|\n",
      "|            12|2024-03-09 11:50:...|516.88|        Transfer|\n",
      "|            13|2024-01-22 16:12:...|615.23|         Deposit|\n",
      "|            14|2024-02-18 19:51:...|119.83|        Transfer|\n",
      "|            15|2024-04-15 10:58:...|630.29|         Deposit|\n",
      "|            16|2024-02-22 06:42:...|923.79|         Deposit|\n",
      "|            17|2024-04-22 06:27:...|832.71|      Withdrawal|\n",
      "|            18|2024-02-14 10:28:...| 73.65|        Transfer|\n",
      "|            19|2024-01-20 09:52:...| 850.8|      Withdrawal|\n",
      "+--------------+--------------------+------+----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Reodering the column\n",
    "transaction = transaction.select('transaction_id', 'Transaction_Date', 'Amount', 'Transaction_Type' )\n",
    "transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd43c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|customer_id|     Customer_Name|    Customer_Address|       Customer_City|Customer_State|    Customer_Country|               Email|        Phone_Number|\n",
      "+-----------+------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|          0|    Miguel Leonard|262 Beck Expressw...|             Unknown| West Virginia|             Eritrea| zweaver@example.net|             Unknown|\n",
      "|          1|           Unknown|             Unknown|         Evanchester|        Oregon|             Uruguay|             Unknown| (384)778-9942x91236|\n",
      "|          2|    Michael Murphy|894 Williams Ridg...|       Dominguezview|      New York|              Sweden|kristinstanley@ex...|+1-693-739-2204x8851|\n",
      "|          3|    Tina Gutierrez|    415 Taylor Knoll|           Donnastad|South Carolina|             Unknown|sarabrooks@exampl...|  623-933-0431x87174|\n",
      "|          4|      Kylie Adkins|    435 Nicole Curve|             Unknown|     Louisiana|             Unknown|davisronald@examp...|  (404)814-4457x1451|\n",
      "|          5|           Unknown|   36415 Chavez Oval|      Alejandroville|     Louisiana|                Peru|solomonheather@ex...|    894.974.9717x529|\n",
      "|          6|       Brian Glenn|505 Mcdowell Gard...|South Christinech...|  South Dakota|             Lesotho|bcabrera@example.net|001-962-928-1897x...|\n",
      "|          7|       Gary Archer| 0930 Michael Stream|            Boydport|North Carolina|Palestinian Terri...|kennethscott@exam...|       (713)965-2955|\n",
      "|          8|       Chase Smith|   156 Jeffrey Union|   Lake Nataliemouth|       Arizona|     North Macedonia|davidreeves@examp...|             Unknown|\n",
      "|          9|Mr. Gary Brooks II|   786 Mccarthy Fall|         Traceymouth|         Idaho|            Kiribati|ellislouis@exampl...|          8963308214|\n",
      "|         10|        Zoe Wilson|4575 Michael Harb...|        Alexanderton|        Alaska|           Venezuela|             Unknown|             Unknown|\n",
      "|         11|           Unknown|     0281 Diaz Locks|        Freemanburgh|        Oregon|Bouvet Island (Bo...|ygonzalez@example...|001-877-339-2451x...|\n",
      "|         12|           Unknown|4339 Hall Street ...|           Dianatown|       Unknown|             Moldova|robinsonbrian@exa...|             Unknown|\n",
      "|         13|    Zachary Grimes|42686 Jessica Loc...|           Johnshire|      Virginia|             Tunisia|vanessa93@example...|       (464)800-5827|\n",
      "|         14| Jessica Zimmerman|    15565 Reed Wells|       East Johnland|       Florida|                Chad|awilliams@example...|  664.840.4619x25293|\n",
      "|         15|     Tammy Simmons|997 Flores Locks ...|            Woodston|      Nebraska|                Oman| udecker@example.com|+1-853-249-6586x8571|\n",
      "|         16| Virginia Galloway|236 Harris Trail ...|       Lake Benjamin|      Michigan|United States of ...|   mross@example.com|   987.344.8192x3020|\n",
      "|         17|   Elizabeth Kline|  8389 Nicholson Run|           Jeffmouth|    Washington|             Liberia|matthewtaylor@exa...|    794-659-1946x128|\n",
      "|         18|     Matthew Costa|100 Crystal Gatew...|     Lake Leslieland|     Louisiana|         Timor-Leste|             Unknown|       (898)827-1111|\n",
      "|         19| Stephanie Simpson|             Unknown|          Murrayland|         Idaho|Northern Mariana ...|jacobgoodman@exam...|    592-707-2311x542|\n",
      "+-----------+------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Customer Table\n",
    "customer = nuga_bank_df_clean.select('Customer_Name','Customer_Address','Customer_City','Customer_State', \\\n",
    "                                     'Customer_Country', 'Email', 'Phone_Number').distinct()\n",
    "\n",
    "# Add id column\n",
    "customer = customer.withColumn('customer_id', monotonically_increasing_id())\n",
    "\n",
    "# Reodering the column\n",
    "customer = customer.select('customer_id', 'Customer_Name','Customer_Address','Customer_City', \\\n",
    "                                     'Customer_State','Customer_Country', 'Email', 'Phone_Number' )\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16633b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "|employee_id|             Company|           Job_Title| Gender|Marital_Status|\n",
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "|          0|         Price Group|             Unknown|   Male|        Single|\n",
      "|          1|Rhodes, King and ...| Trade mark attorney|   Male|       Unknown|\n",
      "|          2|Schmidt, Morgan a...|     Engineer, water| Female|        Single|\n",
      "|          3|       Johnson Group|  Forensic scientist|   Male|       Unknown|\n",
      "|          4|     Phillips-Prince|Production assist...|Unknown|        Single|\n",
      "|          5|      Henry and Sons|Engineer, civil (...| Female|       Married|\n",
      "|          6|Thompson, Johnson...|Exercise physiolo...|  Other|       Unknown|\n",
      "|          7|Hernandez, Johnso...|Forensic psycholo...|Unknown|      Divorced|\n",
      "|          8|Carrillo, Schwart...| Solicitor, Scotland| Female|        Single|\n",
      "|          9|         Olson-Lucas| Magazine journalist|   Male|      Divorced|\n",
      "|         10|        Baxter-Knapp|Designer, televis...| Female|       Unknown|\n",
      "|         11|             Unknown|Programme researc...| Female|      Divorced|\n",
      "|         12|    Newton-Schneider|             Unknown|  Other|      Divorced|\n",
      "|         13|      Suarez-Terrell|            Best boy| Female|      Divorced|\n",
      "|         14|         Stewart LLC|       Archaeologist|   Male|        Single|\n",
      "|         15|Bernard, Sutton a...|Museum/gallery co...|  Other|       Unknown|\n",
      "|         16|Summers, Thornton...|Volunteer coordin...|Unknown|        Single|\n",
      "|         17|     Mcneil-Guerrero|     Ophthalmologist|Unknown|      Divorced|\n",
      "|         18|       Collins-Clark|Producer, televis...| Female|      Divorced|\n",
      "|         19|Lewis, Navarro an...|             Unknown|   Male|        Single|\n",
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Employee Table\n",
    "employee = nuga_bank_df_clean.select('Company','Job_Title','Gender', 'Marital_Status').distinct()\n",
    "\n",
    "# Add id column\n",
    "employee = employee.withColumn('employee_id', monotonically_increasing_id())\n",
    "\n",
    "# Reodering the column\n",
    "employee = employee.select('employee_id', 'Company','Job_Title','Gender', 'Marital_Status')\n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f283be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact table\n",
    "fact_table = nuga_bank_df_clean.join(customer, ['Customer_Name','Customer_Address','Customer_City', \\\n",
    "                                    'Customer_State','Customer_Country', 'Email', 'Phone_Number'], 'left') \\\n",
    "                                        .join(transaction, ['Transaction_Date', 'Amount', \\\n",
    "                                         'Transaction_Type'], 'left') \\\n",
    "                                        .join(employee, ['Company','Job_Title','Gender', \\\n",
    "                                           'Marital_Status'], 'left') \\\n",
    "                                        .select('transaction_id', 'customer_id', 'employee_id', \\\n",
    "                                            'Credit_Card_Number','IBAN','Currency_Code','Random_Number', \\\n",
    "                                            'Category','Group','Is_Active','Last_Updated','Description',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5e6fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-----------+------------------+--------------------+-------------+-------------+--------+-------+---------+--------------------+--------------------+\n",
      "|transaction_id|customer_id|employee_id|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|  Group|Is_Active|        Last_Updated|         Description|\n",
      "+--------------+-----------+-----------+------------------+--------------------+-------------+-------------+--------+-------+---------+--------------------+--------------------+\n",
      "|   34359759045|42949691602|      14483|      630428157006|GB86GLHT381589496...|      Unknown|       5000.0|       A|      Z|       No|2023-10-14 00:47:...|Everything decade...|\n",
      "|   60129602657|60129583452|      45443|    38082745081301|             Unknown|          DJF|          0.0|       A|      Z|      Yes|2021-05-24 05:28:...|Into because end....|\n",
      "|    8589968928|60129572500|      33729|     4021800082481|GB78UBAH195883770...|          LBP|       8097.0|       C|      X|      Yes|2023-07-11 06:06:...|Maybe teacher dee...|\n",
      "|    8590015502|17179947285|      71176|    30448552436896|             Unknown|          ISK|       1108.0|       A|      Y|       No|2022-12-06 03:53:...|Wrong part rest c...|\n",
      "|   42949710387| 8589971882|      28592|   213133896337542|GB07IUUE487965913...|          ISK|          0.0|       D|      Y|       No|2023-08-16 10:32:...|Themselves make ago.|\n",
      "|        107665|34359839584|      87283|  3542448933285876|GB22VSFS474270337...|      Unknown|       6216.0|       A|      X|       No|2020-08-05 03:15:...|Meet PM worry who...|\n",
      "|   17179878388|60129549698|        537|   213186454811670|GB77SPMZ984195063...|          SOS|       7610.0|       C|      X|       No|2021-04-15 13:30:...|Concern remember ...|\n",
      "|   60129587216|42949704054|      29104|  6011177559558424|GB76DNER497098023...|          BZD|       6499.0|       D|      X|  Unknown|2020-02-09 13:11:...|Risk take recent ...|\n",
      "|   60129652878|34359824350|      85904|  6574717409681808|GB58ZWIH992989778...|          LSL|          0.0|       B|      Y|       No|2021-11-04 10:41:...|Any state recogni...|\n",
      "|    8589996138|17179927764|      54426|      639006216746|GB81CQQR004909975...|      Unknown|       8199.0|       B|      Y|       No|2024-03-31 18:51:...|Try step sound. B...|\n",
      "|   25769854944|25769856804|      45959|  2277452848592163|GB36UBKI615026373...|          ARS|       3608.0|       D|      Z|      Yes|2020-08-08 08:43:...|Game fund start w...|\n",
      "|   42949784853|34359846215|      93735|   345333404165465|GB96PUDP643655555...|          DJF|       3059.0|       B|      Z|      Yes|2022-07-22 21:01:...|Really conference...|\n",
      "|   17179939275|      80035|      71679|  6507797348788197|GB31LUCU530383937...|          IRR|       5852.0|       D|      Y|       No|2024-03-09 12:39:...|     Dog peace fish.|\n",
      "|   60129566857|      14819|      21209|      676165997344|             Unknown|          AZN|       6821.0|       A|      Z|       No|2022-07-01 23:12:...|Bag foot store su...|\n",
      "|   60129602656| 8589976944| 8589979681|  6011487394633278|GB21YRPY294696766...|          PAB|       3368.0|       C|      X|       No|2023-07-31 12:32:...|Enter ask remembe...|\n",
      "|   25769873426|25769872242| 8589993427|   180067592769732|GB92JVMY004197871...|          EGP|       7198.0|       A|      Z|      Yes|2023-10-12 22:25:...|Before story prof...|\n",
      "|   25769803777|51539612593| 8589942641|   213112163828334|GB50TJFN039979307...|          SVC|       7382.0|       B|      Z|      Yes|2020-01-19 18:19:...|Great evening so ...|\n",
      "|         47424|17179920314| 8589978110|                 0|GB24FDDZ433609720...|          NIS|       7615.0|       D|      X|       No|2021-04-26 09:55:...|             Unknown|\n",
      "|   60129665405|42949780967| 8590025415|  6595994198943849|             Unknown|          KPW|       5778.0|       A|Unknown|       No|2022-08-17 18:44:...|             Unknown|\n",
      "|   17179955063|51539696397| 8590020298|                 0|GB32LGFL895760023...|          PAB|       8898.0|       A|      Y|      Yes|2021-12-07 15:35:...|Face field coach ...|\n",
      "+--------------+-----------+-----------+------------------+--------------------+-------------+-------------+--------+-------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab090855",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o902.parquet.\n: java.util.concurrent.ExecutionException: Boxed Exception\r\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\t\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\t\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\t\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\t\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# output the transformed data to parquet\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtransaction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset/transaction\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m customer.write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).parquet(\u001b[33m'\u001b[39m\u001b[33mdataset/customer\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m employee.write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).parquet(\u001b[33m'\u001b[39m\u001b[33mdataset/employee\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o902.parquet.\n: java.util.concurrent.ExecutionException: Boxed Exception\r\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\t\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\t\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\t\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\t\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# output the transformed data to parquet\n",
    "transaction.write.mode('overwrite').parquet('dataset/transaction')\n",
    "customer.write.mode('overwrite').parquet('dataset/customer')\n",
    "employee.write.mode('overwrite').parquet('dataset/employee')\n",
    "fact_table.write.mode('overwrite').parquet('dataset/fact_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b48e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the transformed data to csv\n",
    "transaction.repartition(1).write.mode('overwrite').option('header','true').csv(r'dataset/transformeddata/csv/transaction')\n",
    "customer.repartition(1).write.mode('overwrite').option('header','true').csv(r'dataset/transformeddata/csv/customer')\n",
    "employee.repartition(1).write.mode('overwrite').option('header','true').csv(r'dataset/transformeddata/csv/employee')\n",
    "fact_table.repartition(1).write.mode('overwrite').option('header','true').csv(r'dataset/transformeddata/csv/fact_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6814fc83",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o748.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 444.0 failed 1 times, most recent failure: Lost task 1.0 in stage 444.0 (TID 943) (DESKTOP-IVUNJ9J executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convert spark df to pandas df\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m transaction_pd_df = \u001b[43mtransaction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m customer_pd_df = customer.toPandas()\n\u001b[32m      4\u001b[39m employee_pd_df = employee.toPandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:1792\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:197\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    199\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    200\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WINDOWS11\\anaconda3\\envs\\nugabankenv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o748.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 444.0 failed 1 times, most recent failure: Lost task 1.0 in stage 444.0 (TID 943) (DESKTOP-IVUNJ9J executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "# Convert spark df to pandas df\n",
    "transaction_pd_df = transaction.toPandas()\n",
    "customer_pd_df = customer.toPandas()\n",
    "employee_pd_df = employee.toPandas()\n",
    "fact_table_pd_df = fact_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd5127",
   "metadata": {},
   "source": [
    "### Load the dataSET into a Posgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0450f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.10-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Using cached psycopg2-2.9.10-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf25c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12e9b3c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transaction_pd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#Connect tp postgreSQL Server\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m engine.connect() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m#Create tables and load the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[43mtransaction_pd_df\u001b[49m.to_sql(\u001b[33m'\u001b[39m\u001b[33mtransaction\u001b[39m\u001b[33m'\u001b[39m, connection, index=\u001b[38;5;28;01mFalse\u001b[39;00m, if_exists=\u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     23\u001b[39m     customer_pd_df.to_sql(\u001b[33m'\u001b[39m\u001b[33mcustomer\u001b[39m\u001b[33m'\u001b[39m, connection, index=\u001b[38;5;28;01mFalse\u001b[39;00m, if_exists=\u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m     employee_pd_df.to_sql(\u001b[33m'\u001b[39m\u001b[33memployee\u001b[39m\u001b[33m'\u001b[39m, connection, index=\u001b[38;5;28;01mFalse\u001b[39;00m, if_exists=\u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'transaction_pd_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define database connection parameters\n",
    "\n",
    "db_params = {\n",
    "    'username' : 'postgres',\n",
    "    'password' : '123456789',\n",
    "    'host' : 'localhost',\n",
    "    'port' : '5432',\n",
    "    'database' : 'nuga_bank',\n",
    "\n",
    "}\n",
    "\n",
    "# Define the database connection url with db parameters\n",
    "db_url = f\"postgresql://{db_params['username']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\"\n",
    "\n",
    "#Create the database engine with the bd url\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "\n",
    "#Connect tp postgreSQL Server\n",
    "with engine.connect() as connection:\n",
    "    #Create tables and load the data\n",
    "    transaction_pd_df.to_sql('transaction', connection, index=False, if_exists='replace')\n",
    "    customer_pd_df.to_sql('customer', connection, index=False, if_exists='replace')\n",
    "    employee_pd_df.to_sql('employee', connection, index=False, if_exists='replace')\n",
    "    fact_table.to_sql('fact_table', connection, index=False, if_exists='replace')\n",
    "\n",
    "print('Database, tables and data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4fb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nugabankenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
